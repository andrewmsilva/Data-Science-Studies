{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "The purpose of this notebook is to present an example of Topic Modeling usins Latent Dirichlet Distribution (LDA). The dataset used is a list of over one million news headlines published over a period of 15 years. These headlines was sourced from ABC (Australian Broadcasting Corp.) and can be downloaded from [Kaggle](https://www.kaggle.com/therohk/million-headlines/data)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Steps\n",
    "* [The data](#The-data)\n",
    "* [Data pre-processing](#Data-pre-processing)\n",
    "* [Generate Bag of Words](#Generate-Gag-of-Words)\n",
    "* [Generate TF-IDF](#Generate-TF-IDF)\n",
    "* [Generate LDA models](#Generate-LDA-models)\n",
    "    * [Using Bag of Words](#Using-Bag-of-Words)\n",
    "    * [Using TF-IDF](#Using-TF-IDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>publish_date</th>\n",
       "      <th>headline_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20030219</td>\n",
       "      <td>aba decides against community broadcasting lic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20030219</td>\n",
       "      <td>act fire witnesses must be aware of defamation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20030219</td>\n",
       "      <td>a g calls for infrastructure protection summit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20030219</td>\n",
       "      <td>air nz staff in aust strike for pay rise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20030219</td>\n",
       "      <td>air nz strike to affect australian travellers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1186013</th>\n",
       "      <td>20191231</td>\n",
       "      <td>vision of flames approaching corryong in victoria</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1186014</th>\n",
       "      <td>20191231</td>\n",
       "      <td>wa police and government backflip on drug amne...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1186015</th>\n",
       "      <td>20191231</td>\n",
       "      <td>we have fears for their safety: victorian premier</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1186016</th>\n",
       "      <td>20191231</td>\n",
       "      <td>when do the 20s start</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1186017</th>\n",
       "      <td>20191231</td>\n",
       "      <td>yarraville shooting woman dead man critically ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1186018 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         publish_date                                      headline_text\n",
       "0            20030219  aba decides against community broadcasting lic...\n",
       "1            20030219     act fire witnesses must be aware of defamation\n",
       "2            20030219     a g calls for infrastructure protection summit\n",
       "3            20030219           air nz staff in aust strike for pay rise\n",
       "4            20030219      air nz strike to affect australian travellers\n",
       "...               ...                                                ...\n",
       "1186013      20191231  vision of flames approaching corryong in victoria\n",
       "1186014      20191231  wa police and government backflip on drug amne...\n",
       "1186015      20191231  we have fears for their safety: victorian premier\n",
       "1186016      20191231                              when do the 20s start\n",
       "1186017      20191231  yarraville shooting woman dead man critically ...\n",
       "\n",
       "[1186018 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.options.display.max_rows = 10\n",
    "\n",
    "documents = pd.read_csv('data/abcnews-date-text.csv', error_bad_lines=False)\n",
    "# Viewing\n",
    "display(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data pre-processing\n",
    "The data pre-processing is performed following the below steps:\n",
    "* Split the text into sentences and the sentences into words (tokenization);\n",
    "* Lowercase the words and remove punctuation;\n",
    "* Remove words that have fewer than 3 characters;\n",
    "* Remove all stopwords;\n",
    "* Lemmatize words (words in third person are changed to first person and verbs in past and future tenses are changed to present);\n",
    "* Stem words (words are reduced to their root form)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                   [decid, communiti, broadcast, licenc]\n",
       "1                                      [wit, awar, defam]\n",
       "2                  [call, infrastructur, protect, summit]\n",
       "3                             [staff, aust, strike, rise]\n",
       "4                    [strike, affect, australian, travel]\n",
       "                                ...                      \n",
       "1186013     [vision, flame, approach, corryong, victoria]\n",
       "1186014     [polic, govern, backflip, drug, amnesti, bin]\n",
       "1186015                [fear, safeti, victorian, premier]\n",
       "1186016                                           [start]\n",
       "1186017    [yarravill, shoot, woman, dead, critic, injur]\n",
       "Name: headline_text, Length: 1186018, dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.seed(59)\n",
    "\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "\n",
    "import nltk\n",
    "from nltk.stem.porter import *\n",
    "#nltk.download('wordnet')\n",
    "\n",
    "def preprocess(text):\n",
    "    stemmer = nltk.SnowballStemmer('english')\n",
    "    processed_text = []\n",
    "    for token in gensim.utils.simple_preprocess(text):\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
    "            token = stemmer.stem(nltk.WordNetLemmatizer().lemmatize(token, pos='v'))\n",
    "            processed_text.append(token)\n",
    "    return processed_text\n",
    "\n",
    "processed_docs = documents['headline_text'].map(preprocess)\n",
    "# Viewing\n",
    "display(processed_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word 162 \"govt\" appears 1 time(s)\n",
      "Word 240 \"group\" appears 1 time(s)\n",
      "Word 292 \"vote\" appears 1 time(s)\n",
      "Word 589 \"local\" appears 1 time(s)\n",
      "Word 838 \"want\" appears 1 time(s)\n",
      "Word 3567 \"compulsori\" appears 1 time(s)\n",
      "Word 3568 \"ratepay\" appears 1 time(s)\n"
     ]
    }
   ],
   "source": [
    "dictionary = gensim.corpora.Dictionary(processed_docs)\n",
    "dictionary.filter_extremes(no_below=15, no_above=0.5, keep_n=100000)\n",
    "bow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs]\n",
    "# Viewing\n",
    "doc_sample = bow_corpus[4310]\n",
    "for i in range(len(doc_sample)):\n",
    "    print(\"Word {} \\\"{}\\\" appears {} time(s)\".format(\n",
    "        doc_sample[i][0],\n",
    "        dictionary[doc_sample[i][0]],\n",
    "        doc_sample[i][1])\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word 162 \"govt\" has weight 0.25617525269671065\n",
      "Word 240 \"group\" has weight 0.3011111395538523\n",
      "Word 292 \"vote\" has weight 0.33416888830557095\n",
      "Word 589 \"local\" has weight 0.33377677352466983\n",
      "Word 838 \"want\" has weight 0.3121925622107832\n",
      "Word 3567 \"compulsori\" has weight 0.5158075532653446\n",
      "Word 3568 \"ratepay\" has weight 0.5070590825348879\n"
     ]
    }
   ],
   "source": [
    "from gensim import corpora, models\n",
    "from pprint import pprint\n",
    "\n",
    "tfidf = models.TfidfModel(bow_corpus)\n",
    "tfidf_corpus = tfidf[bow_corpus]\n",
    "# Viewing\n",
    "doc_sample = tfidf_corpus[4310]\n",
    "for i in range(len(doc_sample)):\n",
    "    print(\"Word {} \\\"{}\\\" has weight {}\".format(\n",
    "        doc_sample[i][0],\n",
    "        dictionary[doc_sample[i][0]],\n",
    "        doc_sample[i][1])\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate LDA models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: 0.050*\"australia\" + 0.035*\"trump\" + 0.015*\"test\" + 0.014*\"tasmania\" + 0.013*\"final\" + 0.012*\"royal\" + 0.012*\"win\" + 0.011*\"world\" + 0.011*\"lose\" + 0.010*\"street\"\n",
      "\n",
      "Topic 1: 0.044*\"say\" + 0.027*\"elect\" + 0.014*\"state\" + 0.013*\"labor\" + 0.012*\"china\" + 0.012*\"countri\" + 0.011*\"claim\" + 0.011*\"minist\" + 0.010*\"call\" + 0.010*\"deal\"\n",
      "\n",
      "Topic 2: 0.034*\"govern\" + 0.032*\"queensland\" + 0.025*\"south\" + 0.022*\"north\" + 0.021*\"coast\" + 0.019*\"water\" + 0.014*\"west\" + 0.014*\"commiss\" + 0.014*\"gold\" + 0.012*\"flood\"\n",
      "\n",
      "Topic 3: 0.030*\"charg\" + 0.027*\"court\" + 0.025*\"murder\" + 0.023*\"attack\" + 0.020*\"donald\" + 0.018*\"face\" + 0.017*\"alleg\" + 0.016*\"jail\" + 0.015*\"accus\" + 0.014*\"woman\"\n",
      "\n",
      "Topic 4: 0.025*\"year\" + 0.022*\"market\" + 0.018*\"australian\" + 0.015*\"world\" + 0.013*\"rise\" + 0.013*\"high\" + 0.013*\"bank\" + 0.013*\"price\" + 0.013*\"women\" + 0.012*\"power\"\n",
      "\n",
      "Topic 5: 0.029*\"melbourn\" + 0.019*\"miss\" + 0.018*\"island\" + 0.017*\"tasmanian\" + 0.015*\"guilti\" + 0.013*\"death\" + 0.012*\"chines\" + 0.012*\"john\" + 0.012*\"victorian\" + 0.012*\"search\"\n",
      "\n",
      "Topic 6: 0.020*\"chang\" + 0.017*\"help\" + 0.016*\"live\" + 0.013*\"rural\" + 0.012*\"fund\" + 0.012*\"indigen\" + 0.011*\"life\" + 0.011*\"school\" + 0.011*\"abus\" + 0.010*\"farm\"\n",
      "\n",
      "Topic 7: 0.058*\"polic\" + 0.023*\"crash\" + 0.020*\"die\" + 0.019*\"shoot\" + 0.014*\"investig\" + 0.014*\"dead\" + 0.014*\"speak\" + 0.013*\"interview\" + 0.012*\"fall\" + 0.012*\"driver\"\n",
      "\n",
      "Topic 8: 0.028*\"nation\" + 0.026*\"news\" + 0.014*\"meet\" + 0.013*\"park\" + 0.012*\"public\" + 0.012*\"fear\" + 0.012*\"council\" + 0.012*\"video\" + 0.011*\"look\" + 0.011*\"week\"\n",
      "\n",
      "Topic 9: 0.025*\"canberra\" + 0.019*\"bushfir\" + 0.018*\"busi\" + 0.016*\"victoria\" + 0.015*\"darwin\" + 0.015*\"return\" + 0.011*\"concern\" + 0.011*\"port\" + 0.010*\"game\" + 0.010*\"weather\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lda_model_bow = gensim.models.LdaMulticore(\n",
    "    bow_corpus,\n",
    "    num_topics=10,\n",
    "    id2word=dictionary,\n",
    "    passes=2,\n",
    "    workers=2\n",
    ")\n",
    "# Viewing\n",
    "for idx, topic in lda_model_bow.print_topics(-1):\n",
    "    print('Topic {}: {}\\n'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: 0.012*\"rural\" + 0.012*\"countri\" + 0.009*\"govern\" + 0.009*\"hour\" + 0.009*\"health\" + 0.007*\"fund\" + 0.006*\"nation\" + 0.005*\"say\" + 0.005*\"budget\" + 0.005*\"feder\"\n",
      "\n",
      "Topic 1: 0.018*\"crash\" + 0.010*\"die\" + 0.010*\"polic\" + 0.010*\"miss\" + 0.009*\"search\" + 0.009*\"road\" + 0.009*\"death\" + 0.009*\"friday\" + 0.009*\"monday\" + 0.008*\"driver\"\n",
      "\n",
      "Topic 2: 0.007*\"fiji\" + 0.006*\"presid\" + 0.006*\"indonesia\" + 0.006*\"august\" + 0.006*\"syria\" + 0.006*\"marriag\" + 0.006*\"brexit\" + 0.006*\"elect\" + 0.005*\"terror\" + 0.005*\"univers\"\n",
      "\n",
      "Topic 3: 0.012*\"bushfir\" + 0.010*\"weather\" + 0.009*\"hill\" + 0.006*\"explain\" + 0.006*\"social\" + 0.005*\"footag\" + 0.005*\"human\" + 0.005*\"outback\" + 0.005*\"onlin\" + 0.005*\"firefight\"\n",
      "\n",
      "Topic 4: 0.013*\"coast\" + 0.010*\"turnbul\" + 0.009*\"gold\" + 0.009*\"thursday\" + 0.008*\"plead\" + 0.007*\"histori\" + 0.006*\"peter\" + 0.006*\"mother\" + 0.006*\"malcolm\" + 0.005*\"storm\"\n",
      "\n",
      "Topic 5: 0.010*\"scott\" + 0.009*\"pacif\" + 0.009*\"grandstand\" + 0.008*\"march\" + 0.007*\"queensland\" + 0.006*\"leagu\" + 0.005*\"spring\" + 0.005*\"paul\" + 0.005*\"king\" + 0.005*\"australia\"\n",
      "\n",
      "Topic 6: 0.021*\"market\" + 0.019*\"news\" + 0.009*\"climat\" + 0.009*\"share\" + 0.008*\"wall\" + 0.008*\"australian\" + 0.007*\"street\" + 0.006*\"dollar\" + 0.006*\"busi\" + 0.006*\"john\"\n",
      "\n",
      "Topic 7: 0.017*\"charg\" + 0.016*\"murder\" + 0.014*\"polic\" + 0.012*\"court\" + 0.011*\"alleg\" + 0.011*\"drum\" + 0.009*\"jail\" + 0.009*\"sentenc\" + 0.009*\"stori\" + 0.008*\"assault\"\n",
      "\n",
      "Topic 8: 0.020*\"interview\" + 0.013*\"farm\" + 0.009*\"wednesday\" + 0.009*\"michael\" + 0.009*\"drought\" + 0.008*\"farmer\" + 0.008*\"extend\" + 0.007*\"andrew\" + 0.007*\"dairi\" + 0.006*\"age\"\n",
      "\n",
      "Topic 9: 0.022*\"trump\" + 0.012*\"donald\" + 0.010*\"australia\" + 0.009*\"final\" + 0.009*\"world\" + 0.007*\"live\" + 0.006*\"cricket\" + 0.006*\"test\" + 0.006*\"juli\" + 0.005*\"elect\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lda_model_tfidf = gensim.models.LdaMulticore(\n",
    "    tfidf_corpus,\n",
    "    num_topics=10,\n",
    "    id2word=dictionary,\n",
    "    passes=2,\n",
    "    workers=4\n",
    ")\n",
    "# Viewing\n",
    "for idx, topic in lda_model_tfidf.print_topics(-1):\n",
    "    print('Topic {}: {}\\n'.format(idx, topic))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
