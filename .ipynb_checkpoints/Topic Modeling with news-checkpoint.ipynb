{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The data\n",
    "The data set we’ll use is a list of over one million news headlines published over a period of 15 years and can be downloaded from [Kaggle](https://www.kaggle.com/therohk/million-headlines/data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>publish_date</th>\n",
       "      <th>headline_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20030219</td>\n",
       "      <td>aba decides against community broadcasting lic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20030219</td>\n",
       "      <td>act fire witnesses must be aware of defamation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20030219</td>\n",
       "      <td>a g calls for infrastructure protection summit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20030219</td>\n",
       "      <td>air nz staff in aust strike for pay rise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20030219</td>\n",
       "      <td>air nz strike to affect australian travellers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1186013</th>\n",
       "      <td>20191231</td>\n",
       "      <td>vision of flames approaching corryong in victoria</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1186014</th>\n",
       "      <td>20191231</td>\n",
       "      <td>wa police and government backflip on drug amne...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1186015</th>\n",
       "      <td>20191231</td>\n",
       "      <td>we have fears for their safety: victorian premier</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1186016</th>\n",
       "      <td>20191231</td>\n",
       "      <td>when do the 20s start</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1186017</th>\n",
       "      <td>20191231</td>\n",
       "      <td>yarraville shooting woman dead man critically ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1186018 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         publish_date                                      headline_text\n",
       "0            20030219  aba decides against community broadcasting lic...\n",
       "1            20030219     act fire witnesses must be aware of defamation\n",
       "2            20030219     a g calls for infrastructure protection summit\n",
       "3            20030219           air nz staff in aust strike for pay rise\n",
       "4            20030219      air nz strike to affect australian travellers\n",
       "...               ...                                                ...\n",
       "1186013      20191231  vision of flames approaching corryong in victoria\n",
       "1186014      20191231  wa police and government backflip on drug amne...\n",
       "1186015      20191231  we have fears for their safety: victorian premier\n",
       "1186016      20191231                              when do the 20s start\n",
       "1186017      20191231  yarraville shooting woman dead man critically ...\n",
       "\n",
       "[1186018 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.options.display.max_rows = 10\n",
    "\n",
    "documents = pd.read_csv('data/abcnews-date-text.csv', error_bad_lines=False)\n",
    "# Viewing\n",
    "display(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data pre-processing\n",
    "We will perform the following steps:\n",
    "* Tokenization: split the text into sentences and the sentences into words. Lowercase the words and remove punctuation;\n",
    "* Words that have fewer than 3 characters are removed;\n",
    "* All stopwords are removed;\n",
    "* Words are lemmatized — words in third person are changed to first person and verbs in past and future tenses are changed into present;\n",
    "* Words are stemmed — words are reduced to their root form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                   [decid, communiti, broadcast, licenc]\n",
       "1                                      [wit, awar, defam]\n",
       "2                  [call, infrastructur, protect, summit]\n",
       "3                             [staff, aust, strike, rise]\n",
       "4                    [strike, affect, australian, travel]\n",
       "                                ...                      \n",
       "1186013     [vision, flame, approach, corryong, victoria]\n",
       "1186014     [polic, govern, backflip, drug, amnesti, bin]\n",
       "1186015                [fear, safeti, victorian, premier]\n",
       "1186016                                           [start]\n",
       "1186017    [yarravill, shoot, woman, dead, critic, injur]\n",
       "Name: headline_text, Length: 1186018, dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "#nltk.download('wordnet')\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(59)\n",
    "\n",
    "def lemmatize_stemming(text):\n",
    "    stemmer = SnowballStemmer('english')\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
    "\n",
    "def preprocess(text):\n",
    "    result = []\n",
    "    for token in gensim.utils.simple_preprocess(text):\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
    "            result.append(lemmatize_stemming(token))\n",
    "    return result\n",
    "\n",
    "processed_docs = documents['headline_text'].map(preprocess)\n",
    "# Viewing\n",
    "display(processed_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word 162 \"govt\" appears 1 time.\n",
      "Word 240 \"group\" appears 1 time.\n",
      "Word 292 \"vote\" appears 1 time.\n",
      "Word 589 \"local\" appears 1 time.\n",
      "Word 838 \"want\" appears 1 time.\n",
      "Word 3567 \"compulsori\" appears 1 time.\n",
      "Word 3568 \"ratepay\" appears 1 time.\n"
     ]
    }
   ],
   "source": [
    "dictionary = gensim.corpora.Dictionary(processed_docs)\n",
    "dictionary.filter_extremes(no_below=15, no_above=0.5, keep_n=100000)\n",
    "bow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs]\n",
    "# Viewing\n",
    "doc_sample = bow_corpus[4310]\n",
    "for i in range(len(doc_sample)):\n",
    "    print(\"Word {} \\\"{}\\\" appears {} time.\".format(\n",
    "        doc_sample[i][0],\n",
    "        dictionary[doc_sample[i][0]],\n",
    "        doc_sample[i][1])\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.5850076620505259),\n",
      " (1, 0.38947256567331934),\n",
      " (2, 0.4997099083387053),\n",
      " (3, 0.5063271308533074)]\n"
     ]
    }
   ],
   "source": [
    "from gensim import corpora, models\n",
    "from pprint import pprint\n",
    "\n",
    "tfidf = models.TfidfModel(bow_corpus)\n",
    "corpus_tfidf = tfidf[bow_corpus]\n",
    "# Viewing\n",
    "pprint(corpus_tfidf[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate LDA model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: 0.045*\"australian\" + 0.024*\"woman\" + 0.023*\"kill\" + 0.021*\"china\" + 0.020*\"crash\" + 0.019*\"dead\" + 0.017*\"die\" + 0.017*\"year\" + 0.014*\"leav\" + 0.013*\"royal\"\n",
      "\n",
      "Topic 1: 0.026*\"polic\" + 0.025*\"charg\" + 0.023*\"court\" + 0.021*\"murder\" + 0.017*\"donald\" + 0.015*\"face\" + 0.015*\"alleg\" + 0.014*\"death\" + 0.014*\"jail\" + 0.014*\"drug\"\n",
      "\n",
      "Topic 2: 0.044*\"trump\" + 0.043*\"sydney\" + 0.022*\"nation\" + 0.014*\"final\" + 0.013*\"lose\" + 0.012*\"game\" + 0.011*\"open\" + 0.010*\"beat\" + 0.010*\"scott\" + 0.010*\"morrison\"\n",
      "\n",
      "Topic 3: 0.027*\"polic\" + 0.026*\"victoria\" + 0.022*\"tasmania\" + 0.020*\"adelaid\" + 0.020*\"school\" + 0.014*\"tasmanian\" + 0.014*\"countri\" + 0.014*\"speak\" + 0.012*\"children\" + 0.012*\"break\"\n",
      "\n",
      "Topic 4: 0.022*\"stori\" + 0.013*\"rural\" + 0.013*\"health\" + 0.012*\"busi\" + 0.012*\"indigen\" + 0.012*\"price\" + 0.012*\"power\" + 0.011*\"help\" + 0.010*\"drum\" + 0.010*\"servic\"\n",
      "\n",
      "Topic 5: 0.021*\"market\" + 0.020*\"feder\" + 0.018*\"water\" + 0.018*\"miss\" + 0.014*\"street\" + 0.013*\"farm\" + 0.013*\"gippsland\" + 0.011*\"search\" + 0.011*\"wall\" + 0.011*\"announc\"\n",
      "\n",
      "Topic 6: 0.032*\"govern\" + 0.027*\"bushfir\" + 0.018*\"victorian\" + 0.017*\"emerg\" + 0.014*\"farmer\" + 0.011*\"say\" + 0.010*\"want\" + 0.010*\"polit\" + 0.010*\"citi\" + 0.009*\"john\"\n",
      "\n",
      "Topic 7: 0.036*\"news\" + 0.023*\"north\" + 0.018*\"island\" + 0.014*\"test\" + 0.011*\"centr\" + 0.010*\"korea\" + 0.010*\"resid\" + 0.009*\"northern\" + 0.009*\"christma\" + 0.009*\"go\"\n",
      "\n",
      "Topic 8: 0.062*\"australia\" + 0.027*\"queensland\" + 0.024*\"live\" + 0.023*\"shoot\" + 0.023*\"chang\" + 0.018*\"coast\" + 0.017*\"beach\" + 0.014*\"bank\" + 0.013*\"world\" + 0.012*\"gold\"\n",
      "\n",
      "Topic 9: 0.034*\"elect\" + 0.023*\"year\" + 0.023*\"south\" + 0.019*\"perth\" + 0.016*\"brisban\" + 0.016*\"hous\" + 0.013*\"mental\" + 0.012*\"east\" + 0.012*\"releas\" + 0.012*\"start\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lda_model_bow = gensim.models.LdaMulticore(\n",
    "    bow_corpus,\n",
    "    num_topics=10,\n",
    "    id2word=dictionary,\n",
    "    passes=2,\n",
    "    workers=2\n",
    ")\n",
    "# Viewing\n",
    "for idx, topic in lda_model_bow.print_topics(-1):\n",
    "    print('Topic {}: {}\\n'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: 0.011*\"elect\" + 0.011*\"govern\" + 0.007*\"labor\" + 0.007*\"chang\" + 0.007*\"feder\" + 0.007*\"say\" + 0.007*\"fund\" + 0.006*\"climat\" + 0.006*\"budget\" + 0.006*\"liber\"\n",
      "\n",
      "Topic 1: 0.018*\"countri\" + 0.014*\"hour\" + 0.011*\"royal\" + 0.010*\"bushfir\" + 0.009*\"commiss\" + 0.008*\"street\" + 0.007*\"juli\" + 0.007*\"health\" + 0.007*\"mental\" + 0.007*\"march\"\n",
      "\n",
      "Topic 2: 0.009*\"queensland\" + 0.009*\"weather\" + 0.008*\"turnbul\" + 0.008*\"storm\" + 0.007*\"tasmania\" + 0.006*\"andrew\" + 0.006*\"shark\" + 0.006*\"northern\" + 0.006*\"violenc\" + 0.005*\"domest\"\n",
      "\n",
      "Topic 3: 0.018*\"charg\" + 0.017*\"murder\" + 0.014*\"polic\" + 0.013*\"donald\" + 0.012*\"court\" + 0.011*\"alleg\" + 0.010*\"jail\" + 0.009*\"sentenc\" + 0.009*\"assault\" + 0.009*\"woman\"\n",
      "\n",
      "Topic 4: 0.016*\"interview\" + 0.008*\"world\" + 0.008*\"john\" + 0.006*\"rugbi\" + 0.006*\"dairi\" + 0.006*\"august\" + 0.006*\"novemb\" + 0.006*\"extend\" + 0.005*\"australia\" + 0.005*\"america\"\n",
      "\n",
      "Topic 5: 0.022*\"news\" + 0.020*\"market\" + 0.019*\"rural\" + 0.010*\"price\" + 0.008*\"share\" + 0.008*\"busi\" + 0.008*\"michael\" + 0.007*\"david\" + 0.007*\"nation\" + 0.007*\"dollar\"\n",
      "\n",
      "Topic 6: 0.035*\"trump\" + 0.017*\"drum\" + 0.011*\"tuesday\" + 0.007*\"april\" + 0.005*\"attempt\" + 0.005*\"cultur\" + 0.005*\"day\" + 0.005*\"updat\" + 0.004*\"murray\" + 0.004*\"bank\"\n",
      "\n",
      "Topic 7: 0.017*\"crash\" + 0.012*\"kill\" + 0.012*\"die\" + 0.009*\"dead\" + 0.006*\"live\" + 0.006*\"polic\" + 0.006*\"injur\" + 0.006*\"truck\" + 0.006*\"attack\" + 0.006*\"bodi\"\n",
      "\n",
      "Topic 8: 0.011*\"stori\" + 0.009*\"friday\" + 0.008*\"sport\" + 0.008*\"grandstand\" + 0.007*\"morrison\" + 0.006*\"zealand\" + 0.006*\"outback\" + 0.006*\"explain\" + 0.005*\"drought\" + 0.005*\"australian\"\n",
      "\n",
      "Topic 9: 0.011*\"australia\" + 0.008*\"search\" + 0.007*\"cricket\" + 0.006*\"russia\" + 0.006*\"miss\" + 0.006*\"south\" + 0.006*\"test\" + 0.006*\"ash\" + 0.006*\"hobart\" + 0.005*\"mount\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lda_model_tfidf = gensim.models.LdaMulticore(\n",
    "    corpus_tfidf,\n",
    "    num_topics=10,\n",
    "    id2word=dictionary,\n",
    "    passes=2,\n",
    "    workers=4\n",
    ")\n",
    "# Viewing\n",
    "for idx, topic in lda_model_tfidf.print_topics(-1):\n",
    "    print('Topic {}: {}\\n'.format(idx, topic))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
